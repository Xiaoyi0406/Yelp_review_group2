{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the subdataset \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "#read the data\n",
    "def read_data(nrow, filepath):\n",
    "    \n",
    "    with open(filepath) as f:\n",
    "        temp = f.readline()\n",
    "        temp = json.loads(temp.rstrip())\n",
    "        tdata = pd.DataFrame(temp, index = [0])\n",
    "        for n in range(1,nrow):\n",
    "            temp = json.loads(f.readline().rstrip())\n",
    "            tdata = tdata.append(pd.DataFrame(temp, index = [n]))\n",
    "    return tdata\n",
    "\n",
    "#data_df = read_data(10000, '/Users/kzhao46/Downloads/628-2/data/review_train.json')\n",
    "#data_df=pd.read_csv('/Users/kzhao46/Dropbox/628-2/data/Sw.csv',delimiter=',')\n",
    "#data=data_df[data_df['circle']==1]\n",
    "#data.to_csv('/Users/kzhao46/Dropbox/628-2/data/swin.csv', index = False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Mc =  pd.read_csv('/Users/kzhao46/Dropbox/628-2/data/McDonalds.csv',delimiter=',')\n",
    "data_df=Mc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:\n",
      "(11836, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataset:\")\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "Index(['business_id', 'stars', 'text', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names:\")\n",
    "print(data_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of each column:\n",
      "business_id      int64\n",
      "stars          float64\n",
      "text            object\n",
      "date            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Datatype of each column:\")\n",
    "print(data_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few dataset entries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This location really forces you to go through ...</td>\n",
       "      <td>2018-10-04 16:19:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>One star is generous. I love paying for a larg...</td>\n",
       "      <td>2016-01-30 01:07:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sheesh the manners of the employees here are a...</td>\n",
       "      <td>2018-02-01 19:56:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This location deserves a 0 stars!  Awful just ...</td>\n",
       "      <td>2018-08-23 22:24:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Typical McDonald's, located in the food court ...</td>\n",
       "      <td>2014-01-17 13:10:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        47510    1.0  This location really forces you to go through ...   \n",
       "1       108018    1.0  One star is generous. I love paying for a larg...   \n",
       "2       130147    1.0  Sheesh the manners of the employees here are a...   \n",
       "3       165722    1.0  This location deserves a 0 stars!  Awful just ...   \n",
       "4        30203    3.0  Typical McDonald's, located in the food court ...   \n",
       "\n",
       "                  date  \n",
       "0  2018-10-04 16:19:24  \n",
       "1  2016-01-30 01:07:44  \n",
       "2  2018-02-01 19:56:24  \n",
       "3  2018-08-23 22:24:26  \n",
       "4  2014-01-17 13:10:06  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Few dataset entries:\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This location really forces you to go through ...</td>\n",
       "      <td>2018-10-04 16:19:24</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>One star is generous. I love paying for a larg...</td>\n",
       "      <td>2016-01-30 01:07:44</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sheesh the manners of the employees here are a...</td>\n",
       "      <td>2018-02-01 19:56:24</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This location deserves a 0 stars!  Awful just ...</td>\n",
       "      <td>2018-08-23 22:24:26</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Typical McDonald's, located in the food court ...</td>\n",
       "      <td>2014-01-17 13:10:06</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        47510    1.0  This location really forces you to go through ...   \n",
       "1       108018    1.0  One star is generous. I love paying for a larg...   \n",
       "2       130147    1.0  Sheesh the manners of the employees here are a...   \n",
       "3       165722    1.0  This location deserves a 0 stars!  Awful just ...   \n",
       "4        30203    3.0  Typical McDonald's, located in the food court ...   \n",
       "\n",
       "                  date  length  \n",
       "0  2018-10-04 16:19:24     381  \n",
       "1  2016-01-30 01:07:44     340  \n",
       "2  2018-02-01 19:56:24     308  \n",
       "3  2018-08-23 22:24:26     429  \n",
       "4  2014-01-17 13:10:06     789  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new column for the number of words in text\n",
    "data_df['length'] = data_df['text'].apply(len)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d52efdd24ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cor stars vs length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFacetGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "#cor stars vs length\n",
    "graph = sns.FacetGrid(data=data_df,col='stars')\n",
    "graph.map(plt.hist,'length',bins=50,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-121822603039>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-121822603039>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    data_df.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#language\n",
    "#extract the comments do not have language\n",
    "import re\n",
    "from langdetect import detect\n",
    "def not_lang(text):\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)','',text)\n",
    "    if re.sub('[\\W]+','',text) == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "temp=data_df[data_df['text'].apply(not_lang)].index.values\n",
    "data_df.loc[temp,'lang_type'] = 'en'\n",
    "from langdetect import detect\n",
    "for i in range(data_df.shape[0]):\n",
    "    if i in temp:\n",
    "        continue\n",
    "    else:\n",
    "        data_df.loc[i,'lang_type'] = detect(data_df['text'][i])\n",
    "#translate the language\n",
    "data_df['lang_type'].describe()\n",
    "data_df=data_df[data_df['lang_type']=='en']\n",
    "data_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7010683b8d7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#data_df['lang_type'].describe(include=['O'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'你好'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/googletrans-2.3.0-py3.7.egg/googletrans/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLANGCODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/googletrans-2.3.0-py3.7.egg/googletrans/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mYou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mtranslate\u001b[0m \u001b[0mtext\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "#data_df['lang_type'].describe(include=['O'])\n",
    "c='你好'\n",
    "c.encode('utf8')\n",
    "translator.translate(c).text\n",
    "#from translate import Translator\n",
    "#translator= Translator(to_lang=\"German\")\n",
    "#translator.translate('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct acronym\n",
    "import re\n",
    "\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"can\\'t\", \"can not\", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"cannot\", \"can not \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"what\\'s\", \"what is\", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"\\'ve \", \" have \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"n\\'t\", \" not \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"i\\'m\", \"i am \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"\\'re\", \" are \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"\\'d\", \" would \", sen))\n",
    "data_df['text']=data_df['text'].apply(lambda sen: re.sub(r\"\\'ll\", \" will \", sen))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes to long time to correct the spelling error\n",
    "#from textblob import TextBlob \n",
    "#data_df['text'][:20]=data_df['text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#the frist step to get the dict\n",
    "data_dict=data_df\n",
    "\n",
    "data_dict['text'] = data_dict['text'].apply(lambda x: x.lower())\n",
    "#remove the punctuation\n",
    "#data_dict['text'] = data_dict['text'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e93d10a5aac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JJ'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RB'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-e93d10a5aac0>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JJ[*]?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mJJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN[*]?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RB[*]?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "import nltk \n",
    "from nltk import *\n",
    "data_dict['NN'] =None\n",
    "data_dict['JJ']=None\n",
    "data_dict['RB']=None\n",
    "\n",
    "import numpy as np\n",
    "def detect(text):\n",
    "    NN = []\n",
    "    JJ = []\n",
    "    RB = []\n",
    "    text =  word_tokenize(text)\n",
    "    temp = nltk.pos_tag(text)\n",
    "    i = 0\n",
    "    for word, tag in temp:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if re.match('JJ[*]?', tag) != None:\n",
    "            JJ.append(word)\n",
    "        if re.match('NN[*]?', tag) != None:\n",
    "            NN.append(word)\n",
    "        if re.match('RB[*]?', tag) != None:\n",
    "            RB.append(word)\n",
    "    return list([NN, JJ, RB])\n",
    "\n",
    "temp = pd.DataFrame(list(data_dict['text'].apply(detect)))\n",
    "data_dict['NN'], data_dict['JJ'], data_dict['RB'] = temp[:][0], temp[:][1], temp[:][2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location really forces you to go through ...</td>\n",
       "      <td>2018-10-04 16:19:24</td>\n",
       "      <td>381</td>\n",
       "      <td>[location, drive, parking, line, order, burger...</td>\n",
       "      <td>[great, sure, wrong, soggy, bad, bad]</td>\n",
       "      <td>[really, quickly, away]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>one star is generous. i love paying for a larg...</td>\n",
       "      <td>2016-01-30 01:07:44</td>\n",
       "      <td>340</td>\n",
       "      <td>[star, fry, medium, ketchup, napkins, seconds,...</td>\n",
       "      <td>[generous, large, large, first, desperate, busy]</td>\n",
       "      <td>[n't, literally]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sheesh the manners of the employees here are a...</td>\n",
       "      <td>2018-02-01 19:56:24</td>\n",
       "      <td>308</td>\n",
       "      <td>[manners, employees, kind, training, customer,...</td>\n",
       "      <td>[atrocious, sure, nice]</td>\n",
       "      <td>[severely, always, really]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location deserves a 0 stars!  awful just ...</td>\n",
       "      <td>2018-08-23 22:24:26</td>\n",
       "      <td>429</td>\n",
       "      <td>[location, stars, nugget, meal, damn, nuggets,...</td>\n",
       "      <td>[awful, awful, happy, sh, better, last, double...</td>\n",
       "      <td>[n't, together, seriously, always, never]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>typical mcdonald's, located in the food court ...</td>\n",
       "      <td>2014-01-17 13:10:06</td>\n",
       "      <td>789</td>\n",
       "      <td>[mcdonald, food, court, mgm, prices, standing,...</td>\n",
       "      <td>[typical, grand, reasonable, higher, free, ope...</td>\n",
       "      <td>[reasonably, always, anywhere, basically, espe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        47510    1.0  this location really forces you to go through ...   \n",
       "1       108018    1.0  one star is generous. i love paying for a larg...   \n",
       "2       130147    1.0  sheesh the manners of the employees here are a...   \n",
       "3       165722    1.0  this location deserves a 0 stars!  awful just ...   \n",
       "4        30203    3.0  typical mcdonald's, located in the food court ...   \n",
       "\n",
       "                  date  length  \\\n",
       "0  2018-10-04 16:19:24     381   \n",
       "1  2016-01-30 01:07:44     340   \n",
       "2  2018-02-01 19:56:24     308   \n",
       "3  2018-08-23 22:24:26     429   \n",
       "4  2014-01-17 13:10:06     789   \n",
       "\n",
       "                                                  NN  \\\n",
       "0  [location, drive, parking, line, order, burger...   \n",
       "1  [star, fry, medium, ketchup, napkins, seconds,...   \n",
       "2  [manners, employees, kind, training, customer,...   \n",
       "3  [location, stars, nugget, meal, damn, nuggets,...   \n",
       "4  [mcdonald, food, court, mgm, prices, standing,...   \n",
       "\n",
       "                                                  JJ  \\\n",
       "0              [great, sure, wrong, soggy, bad, bad]   \n",
       "1   [generous, large, large, first, desperate, busy]   \n",
       "2                            [atrocious, sure, nice]   \n",
       "3  [awful, awful, happy, sh, better, last, double...   \n",
       "4  [typical, grand, reasonable, higher, free, ope...   \n",
       "\n",
       "                                                  RB  \n",
       "0                            [really, quickly, away]  \n",
       "1                                   [n't, literally]  \n",
       "2                         [severely, always, really]  \n",
       "3          [n't, together, seriously, always, never]  \n",
       "4  [reasonably, always, anywhere, basically, espe...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''data_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#star1=data_dict[data_dict['stars']==1]\n",
    "#star2=data_dict[data_dict['stars']==2]\n",
    "#star3=data_dict[data_dict['stars']==3]\n",
    "#star4=data_dict[data_dict['stars']==4]\n",
    "#star5=data_dict[data_dict['stars']==5]\n",
    "#star1.to_csv('/Users/kzhao46/Dropbox/628-2/data/s1.csv', index = False, encoding='utf-8')\n",
    "#star2.to_csv('/Users/kzhao46/Dropbox/628-2/data/s2.csv', index = False, encoding='utf-8')\n",
    "##star3.to_csv('/Users/kzhao46/Dropbox/628-2/data/s3.csv', index = False, encoding='utf-8')\n",
    "##star5.to_csv('/Users/kzhao46/Dropbox/628-2/data/s5.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location really forces you to go through ...</td>\n",
       "      <td>2018-10-04 16:19:24</td>\n",
       "      <td>381</td>\n",
       "      <td>['location', 'drive', 'parking', 'line', 'orde...</td>\n",
       "      <td>['great', 'sure', 'wrong', 'soggy', 'bad', 'bad']</td>\n",
       "      <td>['really', 'quickly', 'away']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>one star is generous. i love paying for a larg...</td>\n",
       "      <td>2016-01-30 01:07:44</td>\n",
       "      <td>340</td>\n",
       "      <td>['star', 'fry', 'medium', 'ketchup', 'napkins'...</td>\n",
       "      <td>['generous', 'large', 'large', 'first', 'despe...</td>\n",
       "      <td>['literally']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sheesh the manners of the employees here are a...</td>\n",
       "      <td>2018-02-01 19:56:24</td>\n",
       "      <td>308</td>\n",
       "      <td>['manners', 'employees', 'kind', 'training', '...</td>\n",
       "      <td>['atrocious', 'sure', 'nice']</td>\n",
       "      <td>['severely', 'always', 'really']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location deserves a 0 stars!  awful just ...</td>\n",
       "      <td>2018-08-23 22:24:26</td>\n",
       "      <td>429</td>\n",
       "      <td>['location', 'stars', 'nugget', 'meal', 'damn'...</td>\n",
       "      <td>['awful', 'awful', 'happy', 'sh', 'better', 'l...</td>\n",
       "      <td>['together', 'seriously', 'always', 'never']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9325</td>\n",
       "      <td>1.0</td>\n",
       "      <td>terrible service, ordered a qtr pounder , 5 mi...</td>\n",
       "      <td>2018-06-26 18:39:44</td>\n",
       "      <td>167</td>\n",
       "      <td>['service', 'pounder', 'min', 'drive', 'thru',...</td>\n",
       "      <td>['terrible', 'qtr', 'told', 'simple']</td>\n",
       "      <td>['even']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        47510    1.0  this location really forces you to go through ...   \n",
       "1       108018    1.0  one star is generous. i love paying for a larg...   \n",
       "2       130147    1.0  sheesh the manners of the employees here are a...   \n",
       "3       165722    1.0  this location deserves a 0 stars!  awful just ...   \n",
       "4         9325    1.0  terrible service, ordered a qtr pounder , 5 mi...   \n",
       "\n",
       "                  date  length  \\\n",
       "0  2018-10-04 16:19:24     381   \n",
       "1  2016-01-30 01:07:44     340   \n",
       "2  2018-02-01 19:56:24     308   \n",
       "3  2018-08-23 22:24:26     429   \n",
       "4  2018-06-26 18:39:44     167   \n",
       "\n",
       "                                                  NN  \\\n",
       "0  ['location', 'drive', 'parking', 'line', 'orde...   \n",
       "1  ['star', 'fry', 'medium', 'ketchup', 'napkins'...   \n",
       "2  ['manners', 'employees', 'kind', 'training', '...   \n",
       "3  ['location', 'stars', 'nugget', 'meal', 'damn'...   \n",
       "4  ['service', 'pounder', 'min', 'drive', 'thru',...   \n",
       "\n",
       "                                                  JJ  \\\n",
       "0  ['great', 'sure', 'wrong', 'soggy', 'bad', 'bad']   \n",
       "1  ['generous', 'large', 'large', 'first', 'despe...   \n",
       "2                      ['atrocious', 'sure', 'nice']   \n",
       "3  ['awful', 'awful', 'happy', 'sh', 'better', 'l...   \n",
       "4              ['terrible', 'qtr', 'told', 'simple']   \n",
       "\n",
       "                                             RB  \n",
       "0                 ['really', 'quickly', 'away']  \n",
       "1                                 ['literally']  \n",
       "2              ['severely', 'always', 'really']  \n",
       "3  ['together', 'seriously', 'always', 'never']  \n",
       "4                                      ['even']  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "s1 =  pd.read_csv('/Users/kzhao46/Dropbox/628-2/data/s1.csv',delimiter=',')\n",
    "s1.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NN = []\n",
    "for item in data_dict['NN']:\n",
    "    NN += item\n",
    "#count the words\n",
    "from collections import Counter\n",
    "\n",
    "words=NN\n",
    "# words wrong datatype\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int_n = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "reviews_ints_n = []\n",
    "reviews_ints_n.append([vocab_to_int_n[word] for word in NN])\n",
    "print('Unique words: ', len((vocab_to_int_n)))\n",
    "use=dict(counts.most_common(60))\n",
    "use()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "use\n",
    "name=[]\n",
    "rate=[]\n",
    "for key in use:\n",
    "    name.append(key)\n",
    "    rate.append(use[key]/10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "JJ = []\n",
    "for item in data_dict['JJ']:\n",
    "    JJ += item\n",
    "#count the words\n",
    "from collections import Counter\n",
    "\n",
    "words=JJ\n",
    "# words wrong datatype\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int_j = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "reviews_ints_j = []\n",
    "reviews_ints_j.append([vocab_to_int_j[word] for word in JJ])\n",
    "#常见J\n",
    "print('Unique words: ', len((vocab_to_int_j)))\n",
    "counts.most_common(100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#####################\n",
    "RB = []\n",
    "for item in data_dict['RB']:\n",
    "    RB += item\n",
    "#count the words\n",
    "from collections import Counter\n",
    "\n",
    "words=RB\n",
    "\n",
    "# words wrong datatype\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int_r = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "#常见R\n",
    "print('Unique words: ', len((vocab_to_int_r)))\n",
    "counts.most_common(20)\n",
    "\n",
    "reviews_ints_r = []\n",
    "reviews_ints_r.append([vocab_to_int_r[word] for word in RB])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.style.use('ggplot')\n",
    "tt = ' '.join(NN)\n",
    "np.random.seed(321)\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(tt)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.title('Reviews',size=20)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.style.use('ggplot')\n",
    "tt = ' '.join(JJ)\n",
    "np.random.seed(321)\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(tt)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.title('Reviews',size=20)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.style.use('ggplot')\n",
    "tt = ' '.join(RB)\n",
    "np.random.seed(321)\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "wordcloud = WordCloud(background_color=\"black\").generate(tt)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.title('Reviews',size=20)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: idna",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-563eef31f31d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Lemmatization 改到放最后\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, id)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[0;34m(self, url)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: idna"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Lemmatization 改到放最后\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#Reduce the word to their root form (eg. ones->one)\n",
    "data_df['text']=data_df['text'].apply(lambda sen: [WordNetLemmatizer().lemmatize(x) for x in sen.split()])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('food', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('really', 'RB'),\n",
       " ('great', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but & although\n",
    "#built the dictionary called but and although\n",
    "#but=['but','however','yet','nonetheless','whereas','nevertheless']\n",
    "#although=['although','though','notwithstanding','albeit','withal','natheless']\n",
    "'''\n",
    "text =  word_tokenize(\"The food is really great.\")\n",
    "nltk.pos_tag(text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('food', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "text =  word_tokenize(\"The food is really bad.\")\n",
    "nltk.pos_tag(text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('food', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''text =  word_tokenize(\"The food is the  best.\")\n",
    "nltk.pos_tag(text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>length</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location really forces you to go through ...</td>\n",
       "      <td>2018-10-04 16:19:24</td>\n",
       "      <td>381</td>\n",
       "      <td>[location, drive, parking, line, order, burger...</td>\n",
       "      <td>[great, sure, wrong, soggy, bad, bad]</td>\n",
       "      <td>[really, quickly, away]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>one star is generous. i love paying for a larg...</td>\n",
       "      <td>2016-01-30 01:07:44</td>\n",
       "      <td>340</td>\n",
       "      <td>[star, fry, medium, ketchup, napkins, seconds,...</td>\n",
       "      <td>[generous, large, large, first, desperate, busy]</td>\n",
       "      <td>[n't, literally]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sheesh the manners of the employees here are a...</td>\n",
       "      <td>2018-02-01 19:56:24</td>\n",
       "      <td>308</td>\n",
       "      <td>[manners, employees, kind, training, customer,...</td>\n",
       "      <td>[atrocious, sure, nice]</td>\n",
       "      <td>[severely, always, really]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>165722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this location deserves a 0 stars!  awful just ...</td>\n",
       "      <td>2018-08-23 22:24:26</td>\n",
       "      <td>429</td>\n",
       "      <td>[location, stars, nugget, meal, damn, nuggets,...</td>\n",
       "      <td>[awful, awful, happy, sh, better, last, double...</td>\n",
       "      <td>[n't, together, seriously, always, never]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203</td>\n",
       "      <td>3.0</td>\n",
       "      <td>typical mcdonald's, located in the food court ...</td>\n",
       "      <td>2014-01-17 13:10:06</td>\n",
       "      <td>789</td>\n",
       "      <td>[mcdonald, food, court, mgm, prices, standing,...</td>\n",
       "      <td>[typical, grand, reasonable, higher, free, ope...</td>\n",
       "      <td>[reasonably, always, anywhere, basically, espe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id  stars                                               text  \\\n",
       "0        47510    1.0  this location really forces you to go through ...   \n",
       "1       108018    1.0  one star is generous. i love paying for a larg...   \n",
       "2       130147    1.0  sheesh the manners of the employees here are a...   \n",
       "3       165722    1.0  this location deserves a 0 stars!  awful just ...   \n",
       "4        30203    3.0  typical mcdonald's, located in the food court ...   \n",
       "\n",
       "                  date  length  \\\n",
       "0  2018-10-04 16:19:24     381   \n",
       "1  2016-01-30 01:07:44     340   \n",
       "2  2018-02-01 19:56:24     308   \n",
       "3  2018-08-23 22:24:26     429   \n",
       "4  2014-01-17 13:10:06     789   \n",
       "\n",
       "                                                  NN  \\\n",
       "0  [location, drive, parking, line, order, burger...   \n",
       "1  [star, fry, medium, ketchup, napkins, seconds,...   \n",
       "2  [manners, employees, kind, training, customer,...   \n",
       "3  [location, stars, nugget, meal, damn, nuggets,...   \n",
       "4  [mcdonald, food, court, mgm, prices, standing,...   \n",
       "\n",
       "                                                  JJ  \\\n",
       "0              [great, sure, wrong, soggy, bad, bad]   \n",
       "1   [generous, large, large, first, desperate, busy]   \n",
       "2                            [atrocious, sure, nice]   \n",
       "3  [awful, awful, happy, sh, better, last, double...   \n",
       "4  [typical, grand, reasonable, higher, free, ope...   \n",
       "\n",
       "                                                  RB  \n",
       "0                            [really, quickly, away]  \n",
       "1                                   [n't, literally]  \n",
       "2                         [severely, always, really]  \n",
       "3          [n't, together, seriously, always, never]  \n",
       "4  [reasonably, always, anywhere, basically, espe...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'''\n",
    "'''\n",
    "import nltk \n",
    "from nltk import *\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "dic=[\"bathroom\"]\n",
    "def dic_detect(text):\n",
    "    s = []\n",
    "    i = 0\n",
    "    text =  word_tokenize(text)\n",
    "    temp = nltk.pos_tag(text)\n",
    "    for word,tag in temp:\n",
    "        if i>=1:\n",
    "            if re.match('JJ[*]?', temp[i-1][1]) and word in dic :\n",
    "                s.append((temp[i-1][0], word,\"None\"))\n",
    "            if word in dic and re.match('JJ[*]?', temp[i-1][1]) != None :\n",
    "                s.append((word,temp[i-1][0],\"None\"))\n",
    "        \n",
    "        if i>=2:\n",
    "            if  re.match('RB[*]?', temp[i-2][1]) != None and re.match('JJ[*]?', temp[i-1][1]) != None and word in dic:\n",
    "                s.append((word,temp[i-1][0], temp[i-2][0]))\n",
    "            if   temp[i-2][0] in dic and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-2][0], word, temp[i-1][0]))\n",
    "        if i>=2:\n",
    "            if temp[i-2][0] in dic and re.match('VB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-2][0],  word,'None'))\n",
    "        if i>=3:\n",
    "            if  temp[i-3][0] in dic and re.match('VB[*]?', temp[i-2][1]) != None and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-3][0],  word, temp[i-1][0]))\n",
    "        if i>=4:\n",
    "            if temp[i-4][0] in dic and re.match('VB[*]?', temp[i-3][1]) != None and re.match('RB[*]?', temp[i-2][1]) != None and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-4][0], word, temp[i-2][0] +  ' ' + temp[i-1][0]))\n",
    "        i  = i + 1\n",
    "    return s\n",
    "\n",
    "data_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_df['JJ'] = None\n",
    "data_df['NN'] = None\n",
    "data_df['RB'] = None\n",
    "data_df['use']=None\n",
    "#data_df[]\n",
    "\n",
    "data_df['use']=data_df['text'].apply(lambda word: ' '.join(word))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-365ce1342aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#' '.join(data_df['text'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detect'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdic_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-365ce1342aae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sen)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#' '.join(data_df['text'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detect'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdic_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-8fab839ddaaa>\u001b[0m in \u001b[0;36mdic_detect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''#' '.join(data_df['text'])\n",
    "data_df.head()\n",
    "data_df['detect'] = data_df['use'].apply(lambda sen: dic_detect(sen))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_df.head()\n",
    "data_df['NN'] = data_df['detect'].apply(lambda sen: [x[0] for x in sen])\n",
    "data_df['JJ'] = data_df['detect'].apply(lambda sen: [x[1] for x in sen])\n",
    "data_df['RB'] = data_df['detect'].apply(lambda sen: [x[2] for x in sen])\n",
    "data_df.head()\n",
    "flag = []\n",
    "for i in range(len(data_df)):\n",
    "    if data_df['detect'][i]==[]:\n",
    "        flag.append(False)\n",
    "    else:\n",
    "        flag.append(True)\n",
    "toilet = data_df[flag]\n",
    "s1=toilet[toilet['stars']==5]\n",
    "len(s1)\n",
    "s1#1-2 never clean,limited,horrid,moldy,filthy,unclean,dirty,gross,bad,egregious,atrocious,terrible,messy,homeless,worse\n",
    "#6/36 无发判断，lady， public，most\n",
    "#3 clean quick 4/10\n",
    "##too little gross unairconditioned dirty 6/10\n",
    "# 4-5 ultra-modern [clean glamorous nice quick 13/17 ####separate free ] \n",
    "#3/17### dirtiest(comes from questioned review)1/17\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import nltk \n",
    "from nltk import *\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "def recommondation(text):\n",
    "    s = []\n",
    "    i = 0\n",
    "    text =  word_tokenize(text)\n",
    "    temp = nltk.pos_tag(text)\n",
    "    for word,tag in temp:\n",
    "        if i>=1:\n",
    "            if re.match('JJ[*]?', tag) != None and re.match('NN[*]?', temp[i-1][1]) != None :\n",
    "                s.append((temp[i-1][0], word,\"None\"))\n",
    "            if re.match('NN[*]?', tag) != None and re.match('JJ[*]?', temp[i-1][1]) != None :\n",
    "                s.append((word,temp[i-1][0],\"None\"))\n",
    "        \n",
    "        if i>=2:\n",
    "            if  re.match('RB[*]?', temp[i-2][1]) != None and re.match('JJ[*]?', temp[i-1][1]) != None and re.match('NN[*]?', tag) != None:\n",
    "                s.append((word,temp[i-1][0], temp[i-2][0]))\n",
    "            if  re.match('NN[*]?', temp[i-2][1]) != None and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-2][0], word, temp[i-1][0]))\n",
    "        if i>=2:\n",
    "            if re.match('NN[*]?', temp[i-2][1]) != None and re.match('VB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-2][0],  word,'None'))\n",
    "        if i>=3:\n",
    "            if re.match('NN[*]?', temp[i-3][1]) != None and re.match('VB[*]?', temp[i-2][1]) != None and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-3][0],  word, temp[i-1][0]))\n",
    "        if i>=4:\n",
    "            if re.match('NN[*]?', temp[i-4][1]) != None and re.match('VB[*]?', temp[i-3][1]) != None and re.match('RB[*]?', temp[i-2][1]) != None and re.match('RB[*]?', temp[i-1][1]) != None and re.match('JJ[*]?', tag) != None:\n",
    "                s.append((temp[i-4][0], word, temp[i-2][0] +  ' ' + temp[i-1][0]))\n",
    "        i  = i + 1\n",
    "    return s\n",
    "#(NN,JJ,RB)\n",
    "# 需要的词义信息\n",
    "# FW foreign word\n",
    "# CC 并列连词\n",
    "# IN 介词或者从属连词\n",
    "# JJ，JJR， JJS 形容词 比较级，最高级\n",
    "# NN NNS NNP NNPS 名词 复数名词 专有名词 复数专有\n",
    "# RB RBR RBS 副词 比较级 最高级\n",
    "# SYM 符号  \n",
    "# VB VBD VBG VBN VBP VBZ 动词\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bathroom', 'clean', 'really')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommondation('\"bathroom is really clean\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['JJ'] = None\n",
    "data_df['NN'] = None\n",
    "data_df['RB'] = None\n",
    "data_df['recommondation'] = data_df['text'].apply(lambda sen: recommondation(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['NN'] = data_df['recommondation'].apply(lambda sen: [x[0] for x in sen])\n",
    "data_df['JJ'] = data_df['recommondation'].apply(lambda sen: [x[1] for x in sen])\n",
    "data_df['RB'] = data_df['recommondation'].apply(lambda sen: [x[2] for x in sen])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-680039226a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.io.formats.csvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-2b4c5ae0cbf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/kzhao46/Dropbox/628-2/data/Mc_use.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.io.formats.csvs'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "data_df.to_csv('/Users/kzhao46/Dropbox/628-2/data/Mc_use.csv', index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉常用无用词\n",
    "#去掉或替换不常用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#常见\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "counts.most_common(20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#\n",
    "freq = pd.Series(' '.join(data_df['text']).split()).value_counts()[-20:]\n",
    "freq\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "text = 'she is beatiful, kangyi is so cute, although kangyi is bad'\n",
    "text =  word_tokenize(text)\n",
    "temp = nltk.pos_tag(text)\n",
    "temp\n",
    "import string\n",
    "print(string.punctuation)\n",
    "print(although)\n",
    "print(but)\n",
    "re.match('[,.?]', ',')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dict = ['kangyi']\n",
    "def store_nearby(text):\n",
    "    s = []\n",
    "    i = 0\n",
    "    text =  word_tokenize(text)\n",
    "    temp = nltk.pos_tag(text)\n",
    "    for word,tag in temp:\n",
    "        if word in dict:\n",
    "            left  =  max(0, i-4)\n",
    "            right = min(len(temp), i+5)\n",
    "            near = []\n",
    "            for j in range(left, right):\n",
    "                if j == i:\n",
    "                    near.append(temp[j][0])\n",
    "                    continue \n",
    "                print((temp[j][0], temp[j][1]))\n",
    "                if re.match('JJ[*]?',temp[j][1])!=None or re.match('RB[*]?', temp[j][1])!=None or re.match('IN', temp[j][1]):\n",
    "                    near.append(temp[j][0])\n",
    "            print (near)\n",
    "            s.append(near)\n",
    "        i += 1\n",
    "    return s\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "store_nearby('she is beatiful, kangyi is so cute, although kangyi is bad')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def turn(text):\n",
    "    s = []\n",
    "    i = 0\n",
    "    text =  word_tokenize(text)\n",
    "    temp = nltk.pos_tag(text)\n",
    "    l = len(temp)\n",
    "    for i in range(0, l):\n",
    "        if temp[i][0] in although:\n",
    "            left = i\n",
    "            flag = 2\n",
    "            near = []\n",
    "            for right in range(left, l):\n",
    "                if re.match('[,.!?;]', temp[right][0]) != None:\n",
    "                    flag -= 1\n",
    "                if flag == 0:\n",
    "                    break\n",
    "            for j in range(left, right):\n",
    "                if re.match('JJ[*]?',temp[j][1])!=None or re.match('RB[*]?', temp[j][1])!=None or re.match('NN[*]?', temp[j][1]) or temp[j][0] in although or temp[j][0] == ',':\n",
    "                    near.append(temp[j][0])\n",
    "            print(left, right)\n",
    "            s.append(near)\n",
    "        if temp[i][0] in but:\n",
    "            near = []\n",
    "            left = max(0, i-2)\n",
    "            while(left>=0):\n",
    "                if re.match('[,.!?;]', temp[left][0]) != None:\n",
    "                    break\n",
    "                left -= 1\n",
    "            right = min(l, i+2)\n",
    "            while(right<l):\n",
    "                if re.match('[,.!?;]', temp[right][0]) != None:\n",
    "                    break\n",
    "                right += 1\n",
    "            for j in range(left, right):\n",
    "                if re.match('JJ[*]?',temp[j][1])!=None or re.match('RB[*]?', temp[j][1])!=None or re.match('NN[*]?', temp[j][1]) or temp[j][0] in although or temp[j][0] == ',':\n",
    "                    near.append(temp[j][0])\n",
    "            print(left, right)\n",
    "            s.append(near)\n",
    "    return s\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(turn(data_df['text'][1]))\n",
    "print(data_df['text'][1])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
