{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =  pd.read_csv( '/Users/zzz/Dropbox/未命名文件夹/data/res_by_name/newdata/Store/shop_r.csv',encoding = 'latin-1',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11164, 8)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = df['text']\n",
    "#d.to_csv('/Users/zzz/Dropbox/未命名文件夹/data/res_by_name/newdata/Pizza/pizza_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>circle</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>NN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173423</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>0</td>\n",
       "      <td>Had to work super early and swung by to grab a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Had', 'swung', 'vanilla', 'coffee', 'machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173423</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh 7-Eleven my penny candy and late night snac...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['penny', 'candy', 'night', 'snack', 'attack',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173423</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>0</td>\n",
       "      <td>They over charge for items\\n They are never pr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['charge', 'items', 'coffee', 'app', 'beverage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45614</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>0</td>\n",
       "      <td>Best sev in town. Great friendly staff, bakery...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['sev', 'town', 'Great', 'staff', 'bakery', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45614</td>\n",
       "      <td>7-Eleven</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>AB</td>\n",
       "      <td>0</td>\n",
       "      <td>When my uncle was visiting, he once described ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['uncle', 'movie', 'layout', 'flow', 'traffic'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business_id      name     city state  circle  \\\n",
       "0       173423  7-Eleven  Calgary    AB       0   \n",
       "1       173423  7-Eleven  Calgary    AB       0   \n",
       "2       173423  7-Eleven  Calgary    AB       0   \n",
       "3        45614  7-Eleven  Calgary    AB       0   \n",
       "4        45614  7-Eleven  Calgary    AB       0   \n",
       "\n",
       "                                                text  stars  \\\n",
       "0  Had to work super early and swung by to grab a...    1.0   \n",
       "1  Oh 7-Eleven my penny candy and late night snac...    4.0   \n",
       "2  They over charge for items\\n They are never pr...    1.0   \n",
       "3  Best sev in town. Great friendly staff, bakery...    5.0   \n",
       "4  When my uncle was visiting, he once described ...    3.0   \n",
       "\n",
       "                                                  NN  \n",
       "0  ['Had', 'swung', 'vanilla', 'coffee', 'machine...  \n",
       "1  ['penny', 'candy', 'night', 'snack', 'attack',...  \n",
       "2  ['charge', 'items', 'coffee', 'app', 'beverage...  \n",
       "3  ['sev', 'town', 'Great', 'staff', 'bakery', 'f...  \n",
       "4  ['uncle', 'movie', 'layout', 'flow', 'traffic'...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['Had', 'swung', 'vanilla', 'coffee', 'machine...\n",
       "1    ['penny', 'candy', 'night', 'snack', 'attack',...\n",
       "2    ['charge', 'items', 'coffee', 'app', 'beverage...\n",
       "3    ['sev', 'town', 'Great', 'staff', 'bakery', 'f...\n",
       "4    ['uncle', 'movie', 'layout', 'flow', 'traffic'...\n",
       "Name: NN, dtype: object"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.NN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 1\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zzz/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=1, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=5, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7,\n",
    "             learning_method='online', learning_offset=50.0,\n",
    "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
    "             n_jobs=1, n_topics=5, perp_tol=0.1, random_state=0,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "walmart store service time customer people location pharmacy place walgreens employees staff minutes items line manager way day lot prescription gas customers prices times things area mart wal price hours cashier food employee car experience phone home parking money lines order shopping night today stores selection lady thing item hour person counter stock prescriptions job section stars qt card years department review pharmacist stuff work check guy problem register grocery center help drive cashiers business days photo checkout reason insurance wait coffee care products morning station walmarts medication kind vegas week receipt door street groceries star management convenience registers self shelves cart oil woman trip walgreen target house bit issue fact workers rude doctor town water number thank point year product change life attitude couple shop produce cvs worst neighborhood snacks cash girl carts man husband box online drinks company great meds convenient thanks aisles visit kids month locations issues bag electronics family minute stop brand lots quality world photos sale clinic ice course pick tire floor rx medicine needs super drink hand hold pm ones need end return matter min aisle case situation lanes went associate right months managers machine security drug pain eye dog policy corner paper just script auto road room associates las questions sales strip mins information deal plenty places question meat office gift lane credit half coupon hair hell problems look tires purchase reviews bathrooms clean weeks clerk crap spot lack cars beer window bathroom wife son tech shelf supercenter goods id health cards daughter dirty friend rest face battery guys options coupons app size idea soda smile fan alcohol city mess complaint pictures sign yesterday market mind mom lol computer candy christmas bags dogs glasses medications choice air miles pumps dr sense bottle lunch stations cents variety assistance attention inside pizza school refund ghetto mistake type middle supplies joke deals tea word deli milk gentleman head saturday list friday ok hands merchandise transaction entrance option pickup website pills baby worker cost nice eyes ass quiktrip appointment process experiences desk conversation tonight cream valley women hotel inventory folks scripts foods bar garden sunday boxes story difference yelp friends state fault pay cup phoenix bunch picture response pharmacies cosmetics walk discount clothing shoppers email god chicken pump machines refill break slurpee hello game clothes site weekend wow snack bakery supervisor services space answer layout shot midnight date dollar turn contact dollars trash quick fast children shift beauty ladies assistant evening bike doors mother shit restroom match clearance okay com traffic purchases afternoon tomorrow orders sorry restrooms vision oh came waste note chain reasons points party makeup fruit did pharmacists team marts law info im returns kitchen code wally floors love training run rating example key greeter luck clerks charge refills hassle bread sort liquor amazon boyfriend tv sucks trouble child best account got respect express chips damn foot value choices dept police arizona fry cause sandwiches jobs flu paint vehicle isles ca mile costs opinion building pricing skills chance surprise redbox cake kid monday member mall\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-02bd018c9a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[1;32m    398\u001b[0m    \u001b[0mtopic_info\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0m_topic_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m    \u001b[0mtoken_table\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0m_token_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m    \u001b[0mtopic_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_proportion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m    \u001b[0mclient_topic_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_order\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py\u001b[0m in \u001b[0;36m_topic_coordinates\u001b[0;34m(mds, topic_term_dists, topic_proportion)\u001b[0m\n\u001b[1;32m    182\u001b[0m    \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m    \u001b[0mmds_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m    \u001b[0;32massert\u001b[0m \u001b[0mmds_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m    mds_df = pd.DataFrame({'x': mds_res[:,0], 'y': mds_res[:,1], 'topics': range(1, K + 1), \\\n\u001b[1;32m    186\u001b[0m                           'cluster': 1, 'Freq': topic_proportion * 100})\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={\"service\": se})\n",
    "#df.to_csv(\"/Users/zzz/Dropbox/未命名文件夹/data/res_by_name/newdata/Store/shop_se.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(data={\"food\": food})\n",
    "#df.to_csv(\"/Users/zzz/Dropbox/未命名文件夹/data/res_by_name/newdata/Sandwich/sandwich_food.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
